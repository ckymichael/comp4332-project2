{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import node2vec\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_G_from_edges(edges):\n",
    "    edge_dict = dict()\n",
    "    # calculate the count for all the edges\n",
    "    for edge in edges:\n",
    "        edge_key = str(edge[0]) + '_' + str(edge[1])\n",
    "        if edge_key not in edge_dict:\n",
    "            edge_dict[edge_key] = 1\n",
    "        else:\n",
    "            edge_dict[edge_key] += 1\n",
    "    tmp_G = nx.DiGraph()\n",
    "    for edge_key in edge_dict:\n",
    "        weight = edge_dict[edge_key]\n",
    "        # add edges to the graph\n",
    "        tmp_G.add_edge(edge_key.split('_')[0], edge_key.split('_')[1])\n",
    "        # add weights for all the edges\n",
    "        tmp_G[edge_key.split('_')[0]][edge_key.split('_')[1]]['weight'] = weight\n",
    "    return tmp_G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbourhood_score(local_model, node1, node2):\n",
    "    try:\n",
    "        vector1 = local_model.wv.syn0[local_model.wv.index2word.index(node1)]\n",
    "        vector2 = local_model.wv.syn0[local_model.wv.index2word.index(node2)]\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except:\n",
    "        return random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AUC(model, true_edges, false_edges):\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_neighbourhood_score(model, str(edge[0]), str(edge[1]))\n",
    "        true_list.append(1)\n",
    "        prediction_list.append(tmp_score)\n",
    "\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_neighbourhood_score(model, str(edge[0]), str(edge[1]))\n",
    "        true_list.append(0)\n",
    "        prediction_list.append(tmp_score)\n",
    "    y_true = np.array(true_list)\n",
    "    y_scores = np.array(prediction_list)\n",
    "    return roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading the train data.\n",
      "finish loading the valid/test data.\n",
      "Walk iteration:\n",
      "1 / 6\n",
      "2 / 6\n",
      "3 / 6\n",
      "4 / 6\n",
      "5 / 6\n",
      "6 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-03 17:05:15,658 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-04-03 17:05:15,658 : INFO : collecting all words and their counts\n",
      "2019-04-03 17:05:15,658 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-03 17:05:15,684 : INFO : PROGRESS: at sentence #10000, processed 63715 words, keeping 5354 word types\n",
      "2019-04-03 17:05:15,711 : INFO : PROGRESS: at sentence #20000, processed 127297 words, keeping 5354 word types\n",
      "2019-04-03 17:05:15,733 : INFO : PROGRESS: at sentence #30000, processed 191044 words, keeping 5354 word types\n",
      "2019-04-03 17:05:15,739 : INFO : collected 5354 word types from a corpus of 204585 raw words and 32124 sentences\n",
      "2019-04-03 17:05:15,740 : INFO : Loading a fresh vocabulary\n",
      "2019-04-03 17:05:15,754 : INFO : effective_min_count=0 retains 5354 unique words (100% of original 5354, drops 0)\n",
      "2019-04-03 17:05:15,755 : INFO : effective_min_count=0 leaves 204585 word corpus (100% of original 204585, drops 0)\n",
      "2019-04-03 17:05:15,779 : INFO : deleting the raw counts dictionary of 5354 items\n",
      "2019-04-03 17:05:15,780 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2019-04-03 17:05:15,781 : INFO : downsampling leaves estimated 200458 word corpus (98.0% of prior 204585)\n",
      "2019-04-03 17:05:15,813 : INFO : estimated required memory for 5354 words and 250 dimensions: 13385000 bytes\n",
      "2019-04-03 17:05:15,814 : INFO : resetting layer weights\n",
      "2019-04-03 17:05:15,949 : INFO : training model with 4 workers on 5354 vocabulary and 250 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-03 17:05:16,719 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:16,762 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:16,777 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:16,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:16,792 : INFO : EPOCH - 1 : training on 204585 raw words (200466 effective words) took 0.8s, 246095 effective words/s\n",
      "2019-04-03 17:05:17,553 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:17,570 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:17,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:17,616 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:17,617 : INFO : EPOCH - 2 : training on 204585 raw words (200410 effective words) took 0.8s, 246125 effective words/s\n",
      "2019-04-03 17:05:18,488 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:18,488 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:18,509 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:18,543 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:18,544 : INFO : EPOCH - 3 : training on 204585 raw words (200453 effective words) took 0.9s, 219732 effective words/s\n",
      "2019-04-03 17:05:18,545 : INFO : training on a 613755 raw words (601329 effective words) took 2.6s, 231740 effective words/s\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_accuracy: 0.8174194029719808\n",
      "end\n",
      "Walk iteration:\n",
      "1 / 6\n",
      "2 / 6\n",
      "3 / 6\n",
      "4 / 6\n",
      "5 / 6\n",
      "6 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-03 17:05:25,743 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-04-03 17:05:25,743 : INFO : collecting all words and their counts\n",
      "2019-04-03 17:05:25,743 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-03 17:05:25,771 : INFO : PROGRESS: at sentence #10000, processed 72234 words, keeping 5354 word types\n",
      "2019-04-03 17:05:25,792 : INFO : PROGRESS: at sentence #20000, processed 144503 words, keeping 5354 word types\n",
      "2019-04-03 17:05:25,813 : INFO : PROGRESS: at sentence #30000, processed 216709 words, keeping 5354 word types\n",
      "2019-04-03 17:05:25,821 : INFO : collected 5354 word types from a corpus of 232150 raw words and 32124 sentences\n",
      "2019-04-03 17:05:25,822 : INFO : Loading a fresh vocabulary\n",
      "2019-04-03 17:05:25,835 : INFO : effective_min_count=0 retains 5354 unique words (100% of original 5354, drops 0)\n",
      "2019-04-03 17:05:25,836 : INFO : effective_min_count=0 leaves 232150 word corpus (100% of original 232150, drops 0)\n",
      "2019-04-03 17:05:25,856 : INFO : deleting the raw counts dictionary of 5354 items\n",
      "2019-04-03 17:05:25,857 : INFO : sample=0.001 downsamples 24 most-common words\n",
      "2019-04-03 17:05:25,858 : INFO : downsampling leaves estimated 227368 word corpus (97.9% of prior 232150)\n",
      "2019-04-03 17:05:25,876 : INFO : estimated required memory for 5354 words and 250 dimensions: 13385000 bytes\n",
      "2019-04-03 17:05:25,878 : INFO : resetting layer weights\n",
      "2019-04-03 17:05:26,003 : INFO : training model with 4 workers on 5354 vocabulary and 250 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-03 17:05:26,988 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:27,051 : INFO : EPOCH 1 - PROGRESS: at 91.42% examples, 199023 words/s, in_qsize 2, out_qsize 1\n",
      "2019-04-03 17:05:27,051 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:27,071 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:27,092 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:27,093 : INFO : EPOCH - 1 : training on 232150 raw words (227352 effective words) took 1.1s, 211435 effective words/s\n",
      "2019-04-03 17:05:28,128 : INFO : EPOCH 2 - PROGRESS: at 86.15% examples, 193513 words/s, in_qsize 4, out_qsize 0\n",
      "2019-04-03 17:05:28,168 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:28,175 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:28,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:28,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:28,267 : INFO : EPOCH - 2 : training on 232150 raw words (227281 effective words) took 1.2s, 197630 effective words/s\n",
      "2019-04-03 17:05:29,276 : INFO : EPOCH 3 - PROGRESS: at 86.15% examples, 194563 words/s, in_qsize 4, out_qsize 0\n",
      "2019-04-03 17:05:29,304 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:29,334 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:29,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:29,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:29,419 : INFO : EPOCH - 3 : training on 232150 raw words (227326 effective words) took 1.1s, 200248 effective words/s\n",
      "2019-04-03 17:05:29,420 : INFO : training on a 696450 raw words (681959 effective words) took 3.4s, 199634 effective words/s\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_accuracy: 0.8593363155810735\n",
      "end\n",
      "Walk iteration:\n",
      "1 / 6\n",
      "2 / 6\n",
      "3 / 6\n",
      "4 / 6\n",
      "5 / 6\n",
      "6 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-03 17:05:37,659 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-04-03 17:05:37,659 : INFO : collecting all words and their counts\n",
      "2019-04-03 17:05:37,659 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-03 17:05:37,692 : INFO : PROGRESS: at sentence #10000, processed 80989 words, keeping 5354 word types\n",
      "2019-04-03 17:05:37,716 : INFO : PROGRESS: at sentence #20000, processed 161797 words, keeping 5354 word types\n",
      "2019-04-03 17:05:37,744 : INFO : PROGRESS: at sentence #30000, processed 242616 words, keeping 5354 word types\n",
      "2019-04-03 17:05:37,752 : INFO : collected 5354 word types from a corpus of 259872 raw words and 32124 sentences\n",
      "2019-04-03 17:05:37,753 : INFO : Loading a fresh vocabulary\n",
      "2019-04-03 17:05:37,773 : INFO : effective_min_count=0 retains 5354 unique words (100% of original 5354, drops 0)\n",
      "2019-04-03 17:05:37,774 : INFO : effective_min_count=0 leaves 259872 word corpus (100% of original 259872, drops 0)\n",
      "2019-04-03 17:05:37,806 : INFO : deleting the raw counts dictionary of 5354 items\n",
      "2019-04-03 17:05:37,808 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2019-04-03 17:05:37,808 : INFO : downsampling leaves estimated 254228 word corpus (97.8% of prior 259872)\n",
      "2019-04-03 17:05:37,831 : INFO : estimated required memory for 5354 words and 250 dimensions: 13385000 bytes\n",
      "2019-04-03 17:05:37,832 : INFO : resetting layer weights\n",
      "2019-04-03 17:05:37,951 : INFO : training model with 4 workers on 5354 vocabulary and 250 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-03 17:05:38,991 : INFO : EPOCH 1 - PROGRESS: at 73.13% examples, 180264 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-03 17:05:39,165 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:39,222 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:39,289 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:39,337 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:39,339 : INFO : EPOCH - 1 : training on 259872 raw words (254192 effective words) took 1.4s, 185951 effective words/s\n",
      "2019-04-03 17:05:40,383 : INFO : EPOCH 2 - PROGRESS: at 76.95% examples, 189685 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-03 17:05:40,564 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:40,605 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:40,718 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:40,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:40,741 : INFO : EPOCH - 2 : training on 259872 raw words (254265 effective words) took 1.4s, 184803 effective words/s\n",
      "2019-04-03 17:05:41,765 : INFO : EPOCH 3 - PROGRESS: at 53.83% examples, 135889 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-03 17:05:42,199 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-03 17:05:42,320 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-03 17:05:42,359 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-03 17:05:42,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-03 17:05:42,375 : INFO : EPOCH - 3 : training on 259872 raw words (254285 effective words) took 1.6s, 157339 effective words/s\n",
      "2019-04-03 17:05:42,375 : INFO : training on a 779616 raw words (762742 effective words) took 4.4s, 172727 effective words/s\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_accuracy: 0.8597466517168468\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "directed = True\n",
    "p = 1\n",
    "q = 1\n",
    "num_walks = 6\n",
    "walk_lengths = 9\n",
    "dimensions = 230\n",
    "window_size = 10\n",
    "num_workers = 4\n",
    "iterations = 3\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Start to load the raw network\n",
    "\n",
    "train_edges = list()\n",
    "raw_train_data = pandas.read_csv('../data/train.csv')\n",
    "for i, record in raw_train_data.iterrows():\n",
    "    train_edges.append((str(record['head']), str(record['tail'])))\n",
    "\n",
    "print('finish loading the train data.')\n",
    "\n",
    "# Start to load the valid/test data\n",
    "\n",
    "valid_positive_edges = list()\n",
    "valid_negative_edges = list()\n",
    "raw_valid_data = pandas.read_csv('../data/valid.csv')\n",
    "for i, record in raw_valid_data.iterrows():\n",
    "    if record['label']:\n",
    "        valid_positive_edges.append((str(record['head']), str(record['tail'])))\n",
    "    else:\n",
    "        valid_negative_edges.append((str(record['head']), str(record['tail'])))\n",
    "\n",
    "print('finish loading the valid/test data.')\n",
    "\n",
    "train_edges = list(set(train_edges))\n",
    "\n",
    "\n",
    "train_nodes = list()\n",
    "for e in train_edges:\n",
    "    train_nodes.append(e[0])\n",
    "    train_nodes.append(e[1])\n",
    "train_nodes = list(set(train_nodes))\n",
    "\n",
    "auc = list()\n",
    "for walk_length in walk_lengths:\n",
    "    # Create a node2vec object with training edges\n",
    "    G = node2vec.Graph(get_G_from_edges(train_edges), directed, p, q)\n",
    "    # Calculate the probability for the random walk process\n",
    "    G.preprocess_transition_probs()\n",
    "    # Conduct the random walk process\n",
    "    walks = G.simulate_walks(num_walks, walk_length)\n",
    "    # Train the node embeddings with gensim word2vec package\n",
    "    model = Word2Vec(walks, size=dimension, window=window_size, min_count=0, sg=1, workers=num_workers, iter=iterations)\n",
    "    # Save the resulted embeddings (you can use any format you like)\n",
    "    resulted_embeddings = dict()\n",
    "    for i, w in enumerate(model.wv.index2word):\n",
    "        resulted_embeddings[w] = model.wv.syn0[i]\n",
    "    # Test the performance of resulted embeddings with a link prediction task.\n",
    "    tmp_AUC_score = get_AUC(model, valid_positive_edges, valid_negative_edges)\n",
    "\n",
    "    print('tmp_accuracy:', tmp_AUC_score)\n",
    "    auc.append(tmp_AUC_score)\n",
    "\n",
    "    print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0.8597466517168468)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(enumerate(auc),key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8174194029719808, 0.8593363155810735, 0.8597466517168468]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
