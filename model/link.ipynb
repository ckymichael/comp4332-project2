{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import node2vec\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_G_from_edges(edges):\n",
    "    edge_dict = dict()\n",
    "    # calculate the count for all the edges\n",
    "    for edge in edges:\n",
    "        edge_key = str(edge[0]) + '_' + str(edge[1])\n",
    "        if edge_key not in edge_dict:\n",
    "            edge_dict[edge_key] = 1\n",
    "        else:\n",
    "            edge_dict[edge_key] += 1\n",
    "    tmp_G = nx.DiGraph()\n",
    "    for edge_key in edge_dict:\n",
    "        weight = edge_dict[edge_key]\n",
    "        # add edges to the graph\n",
    "        tmp_G.add_edge(edge_key.split('_')[0], edge_key.split('_')[1])\n",
    "        # add weights for all the edges\n",
    "        tmp_G[edge_key.split('_')[0]][edge_key.split('_')[1]]['weight'] = weight\n",
    "    return tmp_G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbourhood_score(local_model, node1, node2):\n",
    "    try:\n",
    "        vector1 = local_model.wv.syn0[local_model.wv.index2word.index(node1)]\n",
    "        vector2 = local_model.wv.syn0[local_model.wv.index2word.index(node2)]\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except:\n",
    "        return random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AUC(model, true_edges, false_edges):\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_neighbourhood_score(model, str(edge[0]), str(edge[1]))\n",
    "        true_list.append(1)\n",
    "        prediction_list.append(tmp_score)\n",
    "\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_neighbourhood_score(model, str(edge[0]), str(edge[1]))\n",
    "        true_list.append(0)\n",
    "        prediction_list.append(tmp_score)\n",
    "    y_true = np.array(true_list)\n",
    "    y_scores = np.array(prediction_list)\n",
    "    return roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading the train data.\n",
      "finish loading the valid/test data.\n",
      "Walk iteration:\n",
      "1 / 6\n",
      "2 / 6\n",
      "3 / 6\n",
      "4 / 6\n",
      "5 / 6\n",
      "6 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-05 01:53:23,440 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-04-05 01:53:23,440 : INFO : collecting all words and their counts\n",
      "2019-04-05 01:53:23,440 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-05 01:53:23,468 : INFO : PROGRESS: at sentence #10000, processed 80911 words, keeping 5354 word types\n",
      "2019-04-05 01:53:23,504 : INFO : PROGRESS: at sentence #20000, processed 161832 words, keeping 5354 word types\n",
      "2019-04-05 01:53:23,532 : INFO : PROGRESS: at sentence #30000, processed 242718 words, keeping 5354 word types\n",
      "2019-04-05 01:53:23,536 : INFO : collected 5354 word types from a corpus of 259948 raw words and 32124 sentences\n",
      "2019-04-05 01:53:23,540 : INFO : Loading a fresh vocabulary\n",
      "2019-04-05 01:53:23,556 : INFO : effective_min_count=0 retains 5354 unique words (100% of original 5354, drops 0)\n",
      "2019-04-05 01:53:23,560 : INFO : effective_min_count=0 leaves 259948 word corpus (100% of original 259948, drops 0)\n",
      "2019-04-05 01:53:23,588 : INFO : deleting the raw counts dictionary of 5354 items\n",
      "2019-04-05 01:53:23,588 : INFO : sample=0.001 downsamples 19 most-common words\n",
      "2019-04-05 01:53:23,588 : INFO : downsampling leaves estimated 254314 word corpus (97.8% of prior 259948)\n",
      "2019-04-05 01:53:23,620 : INFO : estimated required memory for 5354 words and 230 dimensions: 12528360 bytes\n",
      "2019-04-05 01:53:23,620 : INFO : resetting layer weights\n",
      "2019-04-05 01:53:23,760 : INFO : training model with 4 workers on 5354 vocabulary and 230 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-05 01:53:24,600 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-05 01:53:24,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-05 01:53:24,732 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-05 01:53:24,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-05 01:53:24,748 : INFO : EPOCH - 1 : training on 259948 raw words (254326 effective words) took 1.0s, 260871 effective words/s\n",
      "2019-04-05 01:53:25,512 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-05 01:53:25,536 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-05 01:53:25,608 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-05 01:53:25,652 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-05 01:53:25,652 : INFO : EPOCH - 2 : training on 259948 raw words (254296 effective words) took 0.9s, 286927 effective words/s\n",
      "2019-04-05 01:53:26,388 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-05 01:53:26,392 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-05 01:53:26,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-05 01:53:26,492 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-05 01:53:26,496 : INFO : EPOCH - 3 : training on 259948 raw words (254393 effective words) took 0.8s, 311112 effective words/s\n",
      "2019-04-05 01:53:26,496 : INFO : training on a 779844 raw words (763015 effective words) took 2.7s, 278913 effective words/s\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Users\\Cheung Kin Yi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "2019-04-05 01:53:27,252 : INFO : storing 5354x230 projection weights into ../data/test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_accuracy: 0.8652460111103337\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILENAME = '../data/test.csv'\n",
    "\n",
    "directed = True\n",
    "p = 1\n",
    "q = 1\n",
    "num_walks = 6\n",
    "walk_length = 9\n",
    "dimension = 230\n",
    "window_size = 10\n",
    "num_workers = 4\n",
    "iterations = 3\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Start to load the raw network\n",
    "\n",
    "train_edges = list()\n",
    "raw_train_data = pandas.read_csv('../data/train.csv')\n",
    "for i, record in raw_train_data.iterrows():\n",
    "    train_edges.append((str(record['head']), str(record['tail'])))\n",
    "\n",
    "print('finish loading the train data.')\n",
    "\n",
    "# Start to load the valid/test data\n",
    "\n",
    "valid_positive_edges = list()\n",
    "valid_negative_edges = list()\n",
    "raw_valid_data = pandas.read_csv('../data/valid.csv')\n",
    "for i, record in raw_valid_data.iterrows():\n",
    "    if record['label']:\n",
    "        valid_positive_edges.append((str(record['head']), str(record['tail'])))\n",
    "    else:\n",
    "        valid_negative_edges.append((str(record['head']), str(record['tail'])))\n",
    "\n",
    "print('finish loading the valid/test data.')\n",
    "\n",
    "train_edges = list(set(train_edges))\n",
    "\n",
    "\n",
    "train_nodes = list()\n",
    "for e in train_edges:\n",
    "    train_nodes.append(e[0])\n",
    "    train_nodes.append(e[1])\n",
    "train_nodes = list(set(train_nodes))\n",
    "\n",
    "# Create a node2vec object with training edges\n",
    "G = node2vec.Graph(get_G_from_edges(train_edges), directed, p, q)\n",
    "# Calculate the probability for the random walk process\n",
    "G.preprocess_transition_probs()\n",
    "# Conduct the random walk process\n",
    "walks = G.simulate_walks(num_walks, walk_length)\n",
    "# Train the node embeddings with gensim word2vec package\n",
    "model = Word2Vec(walks, size=dimension, window=window_size, min_count=0, sg=1, workers=num_workers, iter=iterations)\n",
    "# Save the resulted embeddings (you can use any format you like)\n",
    "resulted_embeddings = dict()\n",
    "for i, w in enumerate(model.wv.index2word):\n",
    "    resulted_embeddings[w] = model.wv.syn0[i]\n",
    "# Test the performance of resulted embeddings with a link prediction task.\n",
    "tmp_AUC_score = get_AUC(model, valid_positive_edges, valid_negative_edges)\n",
    "\n",
    "print('tmp_accuracy:', tmp_AUC_score)\n",
    "# Save embeddings for later use\n",
    "model.wv.save_word2vec_format(EMBEDDING_FILENAME)\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
